{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555ff8e7-dea4-4cb4-a7dd-935a9e477f07",
   "metadata": {},
   "source": [
    "# Homework 02:  Linear Regression in Theory and Practice\n",
    "\n",
    "In this homework, you’ll work with a simple synthetic regression dataset generated using scikit-learn’s make_regression function. Synthetic data is a valuable tool for testing modeling workflows, gaining hands-on experience with regression models, and understanding how factors like noise and training set size impact model performance—particularly when measured by metrics like Mean Squared Error (MSE). Because we control how the dataset is generated, we can directly assess how well our model recovers the underlying patterns. You’ll also practice splitting data into training and test sets to evaluate how well your model generalizes to unseen data.\n",
    "\n",
    "Some questions will require you to consult the scikit-learn documentation. These functions are widely used in machine learning, and becoming comfortable with reading documentation and examples is an essential part of developing fluency with the tools.\n",
    "\n",
    "As in the first assignment, some tasks are not graded, but that doesn’t mean they are optional. Skipping these steps will weaken your understanding and reduce the long-term value of your effort. Treat every part of the homework seriously—you’ll thank yourself later when you take on real-world projects and more advanced modules.\n",
    "\n",
    "There are 16 problems, each worth 3 points, and you get 2 points for free (if you complete the assignment!). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebc150f-7b44-40d5-9f7e-47fea0b43ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports and utilities\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import kagglehub\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing,make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from math import isclose\n",
    "\n",
    "# globals\n",
    "\n",
    "random_state = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89551b51-31ba-419b-8ae0-2e6fe33acc39",
   "metadata": {},
   "source": [
    "## Problem One: Generate and Visualize a Simple Univariate Regression Dataset\n",
    "\n",
    "In this problem, you’ll explore scikit-learn’s `make_regression` function, which generates synthetic datasets for testing regression models. You can control key parameters like the number of samples and the noise level (standard deviation of the errors), though some settings—such as the feature value ranges or the true coefficients—are randomly generated or limited in configurability (e.g., you can set the bias, but not the coefficients directly).\n",
    "\n",
    "Despite these quirks, `make_regression` is widely used for prototyping and examining model behavior under controlled conditions.\n",
    "\n",
    "Before you begin, take a moment to read the [scikit-learn documentation for `make_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) to understand its parameters and capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c3b27-2855-4135-b46f-6ab72305018f",
   "metadata": {},
   "source": [
    "### Generate a Univariate Dataset\n",
    "\n",
    "Use `make_regression` to generate a simple univariate regression dataset with the following parameters:\n",
    "\n",
    "* `n_samples=20`\n",
    "* `n_features=1` (implied for univariate)\n",
    "* `noise=20` (this sets the standard deviation of the errors)\n",
    "* `bias=0.5` (this sets the y-intercept)\n",
    "* `random_state=42` (for reproducibility)\n",
    "* `coef=True` (this returns the underlying model’s coefficients)\n",
    "\n",
    "Note that when `coef=True`, the function returns a tuple with three values: `X`, `y`, and the  coefficients (in this case, just the slope). Refer to the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html) to understand the return format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d615117-33e1-4e99-ab60-0d69f1c576a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here (you may add additional cells if you wish)\n",
    "\n",
    "bias = 0.5\n",
    "\n",
    "X, y, coefs = make_regression(n_samples= 20, n_features=1, noise=20, bias=bias, random_state=42, coef=True)\n",
    "\n",
    "lb = np.min(X)\n",
    "ub = np.max(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae585720",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418eab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57810167-568f-4db8-b45e-c13656ac976d",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "**TODO:** Set `a1a` to the range of `X`, i.e., a tuple `(lb,ub)` (or array) where `lb` is the smallest value in `X` and `ub` is the largest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5888ef63-db9e-4cbf-b9ba-9a1d26f0485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "a1a = lb, ub   # Replace 0,0 with an expression returning the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc75d1-0bf9-412d-978c-742428d4c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a1a = {a1a}')                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392827f-5cfe-4c56-b543-c23a90dfdbb1",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "**TODO:** Set `a1b` to the slope of the underlying model.\n",
    "\n",
    "Hint: `make_regression` will return the coefficients as an array with one fewer dimensions than `X`; in this case, it is a 0-dimension array whose shape is `()`.  Weird, I know, but this is unusual.  You can pretend it is just a float. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a70d4-0984-4077-a0b4-dc21ceb48347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "a1b = coefs   # Replace 0 with an expression returning the slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46ae45-0daf-4559-8af0-4bb75f098780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a1b = {a1b:.4f}')           # Will print to 4 decimal places        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c85b03a-ff06-4063-82de-d0a138805551",
   "metadata": {},
   "source": [
    "### Visualizing the Data (Nothing for you to do but think about it!)\n",
    "\n",
    "The following will create a plot of the dataset, with the regression line (given by the bias and the slope).\n",
    "\n",
    "**Note:** You will need to complete the previous cell for this to be correct! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e82444-4eda-4697-b5af-73b25193d991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the data points and the regression line\n",
    "\n",
    "if a1b != 0:        # If you answered the previous questions!\n",
    "\n",
    "    bias = 0.5\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X, y, color='blue',label=\"Data points\",marker='.')\n",
    "    plt.plot(a1a,[bias+a1b*a1a[0],bias+a1b*a1a[1]], color=\"grey\", alpha=0.3,label=\"Underlying Model\",linestyle='--')\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Univariate Regression Dataset\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac067b7-163e-4b88-8acb-39bb8bf6215c",
   "metadata": {},
   "source": [
    "## Problem Two: Run Linear Regression on the Data Set and Evaluate the Results\n",
    "\n",
    "Now we will use `sklearn`'s `LinearRegression` model to create a model from the dataset. Of course, the **underlying model** has already been\n",
    "created, but your linear regression won't know that, and it has to determine the best model given the data samples it has.\n",
    "\n",
    "### Part A: Create and Evaluate a Linear Model\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "- Create a linear regression model called simply `model` and train it on `X,y`. \n",
    "- Set `a2a` to the bias/y-intercept of the model.            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af89ed8-d9b3-4993-b599-80e4abb84f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "a2a = model.intercept_   # Replace 0 with an expression returning the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db68bb-dd89-48b6-8664-2eb26f56d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a2a = {a2a:.4f}')           # Will print to 4 decimal places        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedfd0e-f980-47de-a054-b4f3d2157ceb",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "- Set `a2b` to the slope of the model.\n",
    "- Hint: The coefficients are returned as a 1-dimensional array (unlike `make_regression`!), so you'll need to turn a 1-element array into a scalar. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec340411-8fdb-4a39-8ffa-190877869b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "a2b = model.coef_[0]   # Replace 0 with an expression returning the slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338f6ec-c81d-4b34-abe0-736bd028aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a2b = {a2b:.4f}')           # Will print to 4 decimal places        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a35d0-985a-40ef-9de4-3b9656d6f928",
   "metadata": {},
   "source": [
    "### Part C\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "- Set `a2c` to the **training MSE** of the model on the dataset.\n",
    "- Hint: generate an array `y_pred` by using the model to predict the targets from the original `X`, then calculate the mean squared error using the appropriate `sklearn` function.  (Now might be a good time to look at that first code cell to see what I imported for you.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4106016-0a2a-4e9d-addd-6166cc42698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "a2c = mean_squared_error(y, y_pred)   # Replace 0 with an expression returning the training mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a0a5d-c1cf-4f20-9686-1712341b92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a2c = {a2c:.4f}')           # Will print to 4 decimal places        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09992b2a-b906-4b36-9b5d-14a8ea1d3aa0",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "**TODO:** Set `a2d` to the coefficient of determination ($R^2$) of the model (read the docs!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f677126-49de-457b-9492-9fd7f8544096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "a2d = model.score(X,y)   # Replace 0 with an expression returning the r2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b50d99-5e82-4165-8ddd-02a213aee119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a2d = {a2d:.4f}')           # Will print to 4 decimal places        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba1931-cf05-470d-886f-0342ac276b47",
   "metadata": {},
   "source": [
    "### Visualizing The Model (Nothing for you to do but think about it!)\n",
    "\n",
    "We now provide a visualization of the regression line by  adding a plot of the model's regression line in red. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dbe90f-599c-4f7e-ac29-c93e7d90c6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the data points and the regression line\n",
    "\n",
    "if a2d != 0:     # If you answered the previous questions!\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X, y, color='blue',label=\"Data points\",marker='.')\n",
    "    plt.plot(a1a,[bias+a1b*a1a[0],bias+a1b*a1a[1]], color=\"grey\", alpha=0.3,label=\"Underlying Model\",linestyle='--')\n",
    "    plt.plot(a1a,[model.intercept_+model.coef_[0]*a1a[0],model.intercept_+model.coef_[0]*a1a[1]], color=\"red\", label=\"Regression Model\")\n",
    "    \n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Univariate Regression Dataset\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Just some stats\n",
    "    print(f'Underlying model = {np.around(bias,4)} + {np.around(a1b,4)} * x')\n",
    "    print(f'Linear model     = {np.around(model.intercept_,4)} + {np.around(model.coef_[0],4)} * x')\n",
    "    print(f'MSE              = {mean_squared_error(y,y_pred):.4f}')\n",
    "    print(f'R^2              = {np.around(model.score(X, y),4):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec60770f-ad7d-4552-aaae-6b550a3a3de6",
   "metadata": {},
   "source": [
    "### Pause and Ponder (no need to write answers, just think about these):  \n",
    "- Why does the linear regression line not match the underlying model?\n",
    "- Which parameters (`bias`, `n_samples`,`noise`) do you think affect how well the regression model matches the actual model?\n",
    "- What changes to these parameters would result in a more accurate match between underlying and regression models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03580409-1d7a-4432-996c-dfd3e64ebe35",
   "metadata": {},
   "source": [
    "## Problem Three:  How well does it generalize?\n",
    "\n",
    "The **most important issue** in making useful models is to ensure that they are able to **generalize to new data from the same domain.**  For example, if you create a model from a housing price dataset, \n",
    "you want it to be able to predict what price could be obtained if you build new houses with particular features. You will learn techniques for judging how well models generalize in\n",
    "the next few lessons, and it will continue to be a crucial issue going forward. \n",
    "\n",
    "For now, since we have the underlying model (which never happens IRL!) we can easily create new data samples with the same characteristics as the set we used for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654a878-92b9-499e-82d2-9b974c0c18ad",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "- Complete the following stub to create a function to create new points to add the data set, with all the same parameters as the underlying model, following these steps:\n",
    "    1. Create a random number within the existing range of`X`, using `np.random.uniform` \n",
    "    2. Use the bias and slope of the underlying model to find the point (x,y) on the regression line (which is the prediction for y given x)\n",
    "    3. Return (x,y)\n",
    "\n",
    "- Test it by running the cell repeatedly to see the results (we are not setting a random seed, so it will generate random answers)\n",
    "\n",
    "- Generate 5 new data points (we'll use these below as a **test set**) and assign them to ndarrays `X_new` and `y_new` \n",
    "- Hint: create a list of pairs and split using `zip(* ...)`\n",
    "\n",
    "- Set `a3a` to the first 2 values in `X_new`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638f205-2489-4825-9e2c-a4759473fcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here (add additional cells if you wish)\n",
    "\n",
    "def generate_new_point(x_range, true_bias, true_slope):\n",
    "    new_x = np.random.uniform(x_range[0], x_range[1])\n",
    "    new_y = true_bias + true_slope * new_x\n",
    "    return new_x, new_y\n",
    "\n",
    "new_data_point = []\n",
    "for _ in range(5):\n",
    "    new_data_point.append(generate_new_point(a1a, bias, a1b))\n",
    "\n",
    "X_new_list, y_new_list = zip(*new_data_point)\n",
    "\n",
    "X_new = np.array(X_new_list).reshape(-1,1)\n",
    "y_new = np.array(y_new_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfdfb7-aa0f-4efe-b424-da781781f98b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "a3a = X_new[:2]   # Replace [] with an expression returning the first two values in X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a366a9-95a9-462d-a9e0-67544a3f9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a3a = {a3a}')                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00962e3a-a9ec-4ecb-a89a-7e063e414747",
   "metadata": {},
   "source": [
    "### Visualize the Data (Nothing for you to do but think about it!)\n",
    "\n",
    "We now add our new data points in green, in addition to what we displayed in Problem Two. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69934305-3665-4f79-8a90-58f29884dff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the data points and the regression line\n",
    "\n",
    "if len(a3a) > 0:        # If you answered the previous questions!\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X, y, color='blue',label=\"Data points\",marker='.')\n",
    "    plt.scatter(X_new, y_new, color='green',label=\"New Data\",marker='x')\n",
    "    plt.plot(a1a,[bias+a1b*a1a[0],bias+a1b*a1a[1]], color=\"grey\", alpha=0.3,label=\"Underlying Model\",linestyle='--')\n",
    "    plt.plot(a1a,[model.intercept_+model.coef_[0]*a1a[0],model.intercept_+model.coef_[0]*a1a[1]], color=\"red\", label=\"Regression Model\")\n",
    "    \n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Univariate Regression Dataset\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2bba25-6c96-4b6c-8d9c-c019beb2eb1d",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "- Assign the MSE on the new data to `a3b` (we'll later call this the \"test MSE\").\n",
    "- Hint: When you \"roll your own\" datasets using ndarrays, you will generally have to reshape them using `.reshape(-1,1)` because `sklearn` models expect a column array, not a \"normal\" array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e62140-6323-4eb8-85cc-aaabab4acf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "y_new_pred = model.predict(X_new)\n",
    "\n",
    "a3b = mean_squared_error(y_new, y_new_pred)   # Replace 0 with an expression returning the mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e1958-e8b9-40a1-85d3-5904b63b8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a3b = {a3b:.4f}')           # Will print to 4 decimal places        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83de384-9323-45e6-bb23-da7087b50573",
   "metadata": {},
   "source": [
    "### Part C:  Let's Compare Metrics\n",
    "\n",
    "We now have three related values:\n",
    "- Noise = standard deviation of \"errors\" between the data and the underlying model \n",
    "- Training MSE of the linear model on the dataset \n",
    "- Testing MSE of the linear model on new data generated with the same parameters as the original dataset\n",
    "\n",
    "**TODO:**  Answer the following multiple-choice problems by assigning the variable to the  **most accurate** statement.\n",
    "\n",
    "#### C1) Comparing the Two MSEs\n",
    "\n",
    "Why might the training MSE be *larger* than the testing MSE in this scenario?\n",
    "\n",
    "1. These should be exactly the same, so there must have been an error somewhere.  \n",
    "2.  With only 20 training points, a few unusual data points (outliers) can increase the average training error; meanwhile, the small test set of 5 points could *by chance* lead to smaller errors overall.  \n",
    "3. There is always a positive bias in the regression line, so it will always be larger. \n",
    "4.  There is very little relationship between these two numbers, so the fact that they are close to each other must be an accident.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a681e3b-f1bf-4a59-90f5-68d37d7feba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "a3c1 = 2              # Replace the 0 with one of 1, 2, 3, 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be20e44-3197-4291-a779-0091a24a0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a3c1 = {a3c1}')                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe387a-b8fe-40ed-8e78-207fd312a633",
   "metadata": {},
   "source": [
    "#### C2) Understanding the Noise Parameter\n",
    " \n",
    "Suppose a dataset has been generated using `make_regression` with `noise = 20`.  \n",
    "\n",
    "Which statement best explains the significance of the noise parameter when interpreting the MSE of models trained on this dataset?  \n",
    "\n",
    "1. The square of the noise parameter, $20^2 = 400$, represents the irreducible noise in the data; no model can achieve an MSE lower than this on average.  \n",
    "2. A noise standard deviation of 20 means the MSE can eventually be reduced to 0 with enough data.  \n",
    "3. MSE measures the *average* absolute errors, so having $\\sigma = 20$ implies the MSE will always equal 20.  \n",
    "4. If the noise is 20, then it’s possible to create a model with training and testing MSEs that are *exactly* 400 with sufficient effort.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be830d-773e-44b2-953f-70a5a983fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "a3c2 =  1              # Replace the 0 with one of 1, 2, 3, 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abcc9b-0ed0-4178-8e80-7310da16596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a3c2 = {a3c2}')             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33222119-9b15-41ba-85c4-e30b6431def8",
   "metadata": {},
   "source": [
    "#### C3) The Role of Dataset Size\n",
    "\n",
    "You fitted a linear model on **20** training points and tested it on **5** new points, both drawn from a process with noise standard deviation 20 (variance 400).  \n",
    "\n",
    "What would happen if we repeated the same experiment with more training/testing points or with even fewer points?  \n",
    "In other words, **how does dataset size affect the measured MSE values?**\n",
    "\n",
    "1.  If the training set is small, the MSE will always be *exactly* 400 for both training and test sets, since there’s too little data to deviate from the noise variance.  \n",
    "\n",
    "2.  Collecting more data actively *lowers* the true noise standard deviation from 20 to something smaller, guaranteeing an MSE below 400.  \n",
    "3. If you have fewer than 30 data points, the training MSE must always exceed 400 and the test MSE must always be *less* than 400.  \n",
    "4. Small sample sizes can cause large swings in MSE, sometimes pushing the training MSE above 400 while letting a tiny test set fall below 400 by chance. With larger datasets, the MSE typically stabilizes closer to 400.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b7d59-ca7a-49f2-8760-482b2cbe3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "a3c3 = 4              # Replace the 0 with one of 1, 2, 3, 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ebc13e-f24a-445f-9f72-dbfd7a466b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a3c3 = {a3c3}')             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b208269-c2e6-4afc-b215-b0488373f95d",
   "metadata": {},
   "source": [
    "## Problem Four: Linear Regression on an Actual Dataset (finally!)\n",
    "\n",
    "Let's consider applying what we have learned to an actual dataset, the Diabetes dataset from Kaggle. This has 10 features and 1 target,\n",
    "so it is an instance of **multiple regression**, however we can train a model almost exactly as we did in the univariate case. \n",
    "\n",
    "After doing a bit of EDA and massaging of the features, we will first consider separate regressions on a single feature of the dataset, and then run multiple regression on the whole set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd4021-b364-4273-b7d1-2fb771859b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Kaggle Diabetes Dataset\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data_diabetes = load_diabetes(as_frame=True)\n",
    "df_diabetes = pd.concat([data_diabetes.data, data_diabetes.target.rename('DiseaseProgression')], axis=1)\n",
    "\n",
    "feature_names = df_diabetes.drop(columns=[\"DiseaseProgression\"]).columns.tolist()\n",
    "\n",
    "df_diabetes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ebe37a-e5ff-4c53-869e-469d2ae7aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a1b49-6f22-40f3-b8dc-ef0ac34e1282",
   "metadata": {},
   "source": [
    "#### Features of the Diabetes Dataset\n",
    "\n",
    "- **age**: age of the patient  \n",
    "- **sex**: gender of the patient  \n",
    "- **bmi**: body mass index (BMI)  \n",
    "- **bp**:  mean blood pressure  \n",
    "- **s1**:  measure of serum cholesterol levels  \n",
    "- **s2**:  measure related to low-density lipoproteins (LDL)  \n",
    "- **s3**:  measure of high-density lipoproteins (HDL)  \n",
    "- **s4**:  measure of total cholesterol-to-HDL ratio  \n",
    "- **s5**:  measure of serum triglycerides  \n",
    "- **s6**:  measure of blood sugar levels  \n",
    "- **DiseaseProgression**: Quantitative measure of diabetes disease progression one year after baseline (target variable)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b696122-c0f4-4667-955d-59c3992b2e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always useful to create histograms of the features when possible; the layout and formatting are sometimes awkward, so\n",
    "# I use the following:\n",
    "\n",
    "df_diabetes.hist(figsize=(12,8), layout=(3,4),grid=False,edgecolor='black')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af78ecc-046a-4d26-a083-49d9312442c4",
   "metadata": {},
   "source": [
    "### Part A\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "- In order to avoid any awkward moments when discussing the dataset, `rename` the column name 'sex' to 'gender'\n",
    "  *in-place* and set the variable `a4a` to a numpy array of the feature/column names.\n",
    "- Hint: if your answer starts `Index(...` then you have a Pandas data structure and not an ndarray as required. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49945129-0b31-4dd3-807b-bc4ca121f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "df_diabetes.rename(columns={'sex':'gender'}, inplace=True)\n",
    "\n",
    "feature_names = df_diabetes.columns\n",
    "\n",
    "a4a = np.array(feature_names)               # Replace the [] with appropriate ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7728b5d-2b0a-427e-9eb5-e7c4d305ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a4a = {a4a}')             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0aac84-381a-4219-9d4e-6f73f07ce0e5",
   "metadata": {},
   "source": [
    "### Part B\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "- Create dataset in the form `X,y` from the dataframe by dropping the last column to create `X`, and just selecting the last column to make `y`\n",
    "\n",
    "- Note: `sklearn`'s models are perfectly happy to work with dataframes, so we can just keep them as such and not convert to ndarrays.  Two advantages are: you don't have to reshape for input to the model, and you keep the feature names in case you need them later, e.g., when doing feature selection. \n",
    "\n",
    "- Set `a4b` to the shape of `X`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951172a-231f-4c98-ae2d-27352614ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "X= df_diabetes.drop(columns=[\"DiseaseProgression\"])\n",
    "y= df_diabetes[\"DiseaseProgression\"]\n",
    "\n",
    "a4b = X.shape                  # Replace 0,0 with the shape of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef911df-593c-436a-a07a-2b078e768af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a4b = {a4b}')                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8321e1f-5ace-4d7b-87e5-11fa8a8b00cd",
   "metadata": {},
   "source": [
    "### Part C: Training and Testing for Generalization\n",
    "\n",
    "We will spend the next two lessons thinking about how to evaluate  models for generalization, but let's try a naive strategy\n",
    "for now:  We will split the dataset into training and testing sets, and see how the model performs on  data it has never seen.\n",
    "The disadvantage of this is that we have less training data, of course!\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "- Use `sklearn`'s `train_test_split` to shuffle `X` and split it into 80% training data and 20% testing data with `random_state=42`\n",
    "- Train a model `model_diabetes` on the training set, and then test it on the same set to find the training MSE. \n",
    "- Assign the training MSE to `a4c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138eaec5-d34c-49ca-95bf-de6d145b13d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "model_diabetes = LinearRegression()\n",
    "model_diabetes.fit(X_Train, y_train)\n",
    "\n",
    "y_train_pred = model_diabetes.predict(X_train)\n",
    "\n",
    "a4c = 0              # Replace the 0 with the training mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af88434c-b89d-441c-af2d-18e24d6a6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a4c = {a4c:.4f}')             # Will print to 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e564bd24-6b7d-43cf-bebe-39911a96b27d",
   "metadata": {},
   "source": [
    "### Part D\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "- Run the model created in Part C on the testing set to determine the test MSE. \n",
    "- Set `a4d` to the test MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8e4ce-fd09-46ed-8049-0fba40313434",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "\n",
    "a4d =  0              # Replace the 0 with the test mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0fa47-b962-4595-8261-19e11b268eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a4d = {a4d:.4f}')         # Will print to 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00cdf3-d776-4503-b4fc-7bea03a8f1b0",
   "metadata": {},
   "source": [
    "### Part E\n",
    "\n",
    "**TODO:** \n",
    "\n",
    "Now try setting the percentage of the test size to different values, perhaps 0.1, 0.2, 0.3, 0.4, and 0.5 and run the above\n",
    "cells and observe the training and testing MSEs. \n",
    "\n",
    "Then choose the best answer below. \n",
    "\n",
    "\n",
    "**How Does the Training‐Set Size Affect MSE?**\n",
    "\n",
    "1.  Both **training MSE** and **testing MSE** remain exactly the same regardless of how many points you use, provided you keep the `random_state` fixed.  \n",
    "2.  Both **training MSE** and **testing MSE** steadily converge to **zero** once you exceed a certain training‐set size threshold (e.g., 30 points).  \n",
    "3.  **Training MSE** usually goes **up** slightly with a bigger training set (it’s harder to fit more points perfectly), but **testing MSE** tends to go **down** (the model generalizes better with more data).  \n",
    "4.  Both **training MSE** and **testing MSE** decrease when the training set grows, because the model memorizes a larger volume of data and thus reduces all errors.\n",
    "\n",
    "\n",
    "**NOTE: Before submitting your homework, set the percentage back to 20% as in the original problem and remember to Run All!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b00d61-4bde-45f6-8e80-370d2b9af233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer here, NOT in the next cell (you may add additional cells if you wish)\n",
    "\n",
    "a4e = 0           # Replace the 0 with one of 1, 2, 3, 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d9300-c02c-4c1e-9350-305d70ddde7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a4e = {a4e}') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
