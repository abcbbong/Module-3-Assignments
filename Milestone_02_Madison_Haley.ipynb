{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on August 3 (with 2-hour grace period) and worth 50 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1 and will complete the coding portion of your project. You will:\n",
    "\n",
    "1. Pick 3 modeling algorithms from those we have studied.\n",
    "2. Evaluate baseline models using default settings.\n",
    "3. Engineer new features and re-evaluate models.\n",
    "4. Use feature selection techniques and re-evaluate.\n",
    "5. Fine-tune for optimal performance.\n",
    "6. Select your best model and report on your results. \n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling. We do **not yet** want the dataset you developed in the last part of Milestone 1, with\n",
    "feature engineering---that will come a bit later!\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will have to redo the scaling step if you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created zillow_cleaned from milestone1 df_3d (cleaned and imputed version)\n",
    "df = pd.read_csv(\"zillow_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 77380 entries, 0 to 77379\n",
      "Data columns (total 37 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   airconditioningtypeid         77380 non-null  float64\n",
      " 1   bathroomcnt                   77380 non-null  float64\n",
      " 2   bedroomcnt                    77380 non-null  float64\n",
      " 3   buildingqualitytypeid         77380 non-null  float64\n",
      " 4   calculatedbathnbr             77380 non-null  float64\n",
      " 5   calculatedfinishedsquarefeet  77380 non-null  float64\n",
      " 6   fips                          77380 non-null  float64\n",
      " 7   fireplacecnt                  77380 non-null  float64\n",
      " 8   fullbathcnt                   77380 non-null  float64\n",
      " 9   garagecarcnt                  77380 non-null  float64\n",
      " 10  garagetotalsqft               77380 non-null  float64\n",
      " 11  hashottuborspa                77380 non-null  int64  \n",
      " 12  heatingorsystemtypeid         77380 non-null  float64\n",
      " 13  latitude                      77380 non-null  float64\n",
      " 14  longitude                     77380 non-null  float64\n",
      " 15  lotsizesquarefeet             77380 non-null  float64\n",
      " 16  poolcnt                       77380 non-null  float64\n",
      " 17  poolsizesum                   77380 non-null  float64\n",
      " 18  pooltypeid10                  77380 non-null  float64\n",
      " 19  pooltypeid2                   77380 non-null  float64\n",
      " 20  pooltypeid7                   77380 non-null  float64\n",
      " 21  propertycountylandusecode     77380 non-null  object \n",
      " 22  propertylandusetypeid         77380 non-null  float64\n",
      " 23  propertyzoningdesc            77380 non-null  object \n",
      " 24  regionidcity                  77380 non-null  float64\n",
      " 25  regionidcounty                77380 non-null  float64\n",
      " 26  regionidneighborhood          77380 non-null  float64\n",
      " 27  regionidzip                   77380 non-null  float64\n",
      " 28  roomcnt                       77380 non-null  float64\n",
      " 29  threequarterbathnbr           77380 non-null  float64\n",
      " 30  unitcnt                       77380 non-null  float64\n",
      " 31  yearbuilt                     77380 non-null  float64\n",
      " 32  numberofstories               77380 non-null  float64\n",
      " 33  fireplaceflag                 77380 non-null  int64  \n",
      " 34  taxdelinquencyflag            77380 non-null  int64  \n",
      " 35  taxdelinquencyyear            77380 non-null  float64\n",
      " 36  taxvaluedollarcnt             77380 non-null  float64\n",
      "dtypes: float64(32), int64(3), object(2)\n",
      "memory usage: 21.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "\n",
    "# y/target\n",
    "y = df['taxvaluedollarcnt']\n",
    "\n",
    "# X/keep only numeric features\n",
    "X = df.drop(columns='taxvaluedollarcnt').select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Picking Three Models and Establishing Baselines [6 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters** for **three** of the models we have worked with this term:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each of the three models:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV MAE Score**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MAE: 249159.10\n",
      "Linear Regression Std Dev of MAE: 2805.81\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# define repeated k-fold CV\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# MAE\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# perform CV\n",
    "mae_scores = cross_val_score(model, X_train_scaled, y_train, scoring=mae_scorer, cv=cv)\n",
    "\n",
    "# convert from negative MAE to positive\n",
    "mae_scores = -mae_scores\n",
    "\n",
    "# results\n",
    "print(f\"Linear Regression MAE: {mae_scores.mean():.2f}\")\n",
    "print(f\"Linear Regression Std Dev of MAE: {mae_scores.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MAE: 192795.36\n",
      "XGBoost Std Dev of MAE: 1891.57\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=None,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# define repeated k-fold CV\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# negative MAE for scoring, then convert\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "mae_scores = cross_val_score(xgb, X_train_scaled, y_train, scoring=mae_scorer, cv=cv)\n",
    "mae_scores = -mae_scores  # Convert to positive MAE\n",
    "\n",
    "# results\n",
    "print(f\"XGBoost MAE: {mae_scores.mean():.2f}\")\n",
    "print(f\"XGBoost Std Dev of MAE: {mae_scores.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression Mean MAE: 192368.96\n",
      "Random Forest Regression Std Dev of MAE: 2525.14\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=random_state,\n",
    "    min_samples_split=2,\n",
    "    max_leaf_nodes=None,  # Set to None for no limit on leaf nodes\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# define repeated k-fold CV\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# negative MAE for scoring, then convert\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "mae_scores = cross_val_score(rfr, X_train_scaled, y_train, scoring=mae_scorer, cv=cv)\n",
    "mae_scores = -mae_scores  # Convert to positive MAE\n",
    "\n",
    "# results\n",
    "print(f\"Random Forest Regression Mean MAE: {mae_scores.mean():.2f}\")\n",
    "print(f\"Random Forest Regression Std Dev of MAE: {mae_scores.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression: mean MAE=249159.10, std=2805.81\n",
      "RandomForest: mean MAE=192368.96, std=2525.14\n",
      "XGBRegressor: mean MAE=192795.36, std=1891.57\n"
     ]
    }
   ],
   "source": [
    "# Model testing with new features\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForest\": RandomForestRegressor(\n",
    "        n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"XGBRegressor\": XGBRegressor(\n",
    "        n_estimators=100, learning_rate=0.1,\n",
    "        max_depth=6, random_state=42)\n",
    "}\n",
    "\n",
    "# Define scorer for MAE (negative for cross_val_score)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Use repeated K-fold cross-validation for robust evaluation\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    neg_mae = cross_val_score(\n",
    "        model, X_train_scaled, y_train,\n",
    "        scoring=mae_scorer, cv=cv, n_jobs=-1\n",
    "    )\n",
    "    mae_scores = -neg_mae  # convert to positive MAE\n",
    "    results[name] = {\n",
    "        'mean_MAE': mae_scores.mean(),\n",
    "        'std_MAE': mae_scores.std()\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}: mean MAE={metrics['mean_MAE']:.2f}, std={metrics['std_MAE']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [3 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which model performed best overall?\n",
    "  - Which was most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [6 pts]\n",
    "\n",
    "Pick **at least three new features** based on your Milestone 1, Part 5, results. You may pick new ones or\n",
    "use the same ones you chose for Milestone 1. \n",
    "\n",
    "Add these features to `X_train` (use your code and/or files from Milestone 1) and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run the 3 models listed above (using default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression: mean MAE=755135763507001.50, std=1239125278601087.75\n",
      "RandomForest: mean MAE=192357.97, std=2293.08\n",
      "XGBRegressor: mean MAE=192848.36, std=1974.00\n"
     ]
    }
   ],
   "source": [
    "# Add as many cells as you need\n",
    "\n",
    "df_engineered = pd.read_csv(\"zillow_engineered.csv\")\n",
    "\n",
    "y = df_engineered['taxvaluedollarcnt']\n",
    "X = df_engineered.drop(columns='taxvaluedollarcnt').select_dtypes(include=['int64','float64']).copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Current year assumption (e.g., assessment year)\n",
    "current_year = 2017\n",
    "\n",
    "# Home age and new-home indicator\n",
    "X_train['home_age'] = current_year - X_train['yearbuilt']\n",
    "X_test['home_age'] = current_year - X_test['yearbuilt']\n",
    "X_train['is_new_home'] = (X_train['home_age'] < 20).astype(int)\n",
    "X_test['is_new_home'] = (X_test['home_age'] < 20).astype(int)\n",
    "\n",
    "# Drop continuous age\n",
    "X_train = X_train.drop(columns=['home_age'])\n",
    "X_test = X_test.drop(columns=['home_age'])\n",
    "\n",
    "# Bathroom-to-bedroom ratio\n",
    "X_train['bath_bed_ratio'] = X_train['bathroomcnt'] / (X_train['bedroomcnt'] + 1e-3)\n",
    "X_test['bath_bed_ratio'] = X_test['bathroomcnt'] / (X_test['bedroomcnt'] + 1e-3)\n",
    "\n",
    "# Sqft per lot\n",
    "X_train['sqft_per_lot'] = X_train['calculatedfinishedsquarefeet'] / (X_train['lotsizesquarefeet'] + 1e-3)\n",
    "X_test['sqft_per_lot'] = X_test['calculatedfinishedsquarefeet'] / (X_test['lotsizesquarefeet'] + 1e-3)\n",
    "\n",
    "# Total rooms\n",
    "X_train['total_rooms'] = X_train['bedroomcnt'] + X_train['bathroomcnt']\n",
    "X_test['total_rooms'] = X_test['bedroomcnt'] + X_test['bathroomcnt']\n",
    "\n",
    "# Log transform of finished square feet (to address skewness)\n",
    "X_train['log_calculatedfinishedsquarefeet'] = np.log1p(X_train['calculatedfinishedsquarefeet'])\n",
    "X_test['log_calculatedfinishedsquarefeet'] = np.log1p(X_test['calculatedfinishedsquarefeet'])\n",
    "\n",
    "# Square of finished square feet\n",
    "X_train['calcsqft_squared'] = X_train['calculatedfinishedsquarefeet'] ** 2\n",
    "X_test['calcsqft_squared'] = X_test['calculatedfinishedsquarefeet'] ** 2\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "# Fit on training data only\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Apply the same transformation to test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model testing with new features\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForest\": RandomForestRegressor(\n",
    "        n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"XGBRegressor\": XGBRegressor(\n",
    "        n_estimators=100, learning_rate=0.1,\n",
    "        max_depth=6, random_state=42)\n",
    "}\n",
    "\n",
    "# Define scorer for MAE (negative for cross_val_score)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Use repeated K-fold cross-validation for robust evaluation\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    neg_mae = cross_val_score(\n",
    "        model, X_train_scaled, y_train,\n",
    "        scoring=mae_scorer, cv=cv, n_jobs=-1\n",
    "    )\n",
    "    mae_scores = -neg_mae  # convert to positive MAE\n",
    "    results[name] = {\n",
    "        'mean_MAE': mae_scores.mean(),\n",
    "        'std_MAE': mae_scores.std()\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name}: mean MAE={metrics['mean_MAE']:.2f}, std={metrics['std_MAE']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [3 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [6 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features (with default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selection: forward, Model: LinearRegression -> CV MAE: 230953.65 ± 3807.34, Test MAE: 229390.47, Features: ['calcsqft_squared', 'latitude', 'buildingqualitytypeid_9.0', 'buildingqualitytypeid_11.0', 'log_calculatedfinishedsquarefeet', 'buildingqualitytypeid_10.0', 'bedroomcnt', 'buildingqualitytypeid_12.0', 'longitude', 'is_new_home', 'hashottuborspa', 'propertylandusetypeid_246.0', 'poolcnt', 'heatingorsystemtypeid_24.0', 'buildingqualitytypeid_7.0', 'propertylandusetypeid_247.0', 'unitcnt_4.0', 'propertylandusetypeid_275.0', 'propertylandusetypeid_265.0', 'heatingorsystemtypeid_18.0', 'airconditioningtypeid_1.0', 'threequarterbathnbr', 'regionidcity', 'lotsizesquarefeet', 'fireplaceflag', 'propertylandusetypeid_263.0', 'airconditioningtypeid_11.0', 'bath_bed_ratio', 'airconditioningtypeid_5.0', 'unitcnt_237.0', 'airconditioningtypeid_13.0']\n",
      "Selection: forward, Model: RandomForest -> CV MAE: 195261.35 ± 2888.41, Test MAE: 192201.96, Features: ['calcsqft_squared', 'latitude', 'buildingqualitytypeid_9.0', 'buildingqualitytypeid_11.0', 'log_calculatedfinishedsquarefeet', 'buildingqualitytypeid_10.0', 'bedroomcnt', 'buildingqualitytypeid_12.0', 'longitude', 'is_new_home', 'hashottuborspa', 'propertylandusetypeid_246.0', 'poolcnt', 'heatingorsystemtypeid_24.0', 'buildingqualitytypeid_7.0', 'propertylandusetypeid_247.0', 'unitcnt_4.0', 'propertylandusetypeid_275.0', 'propertylandusetypeid_265.0', 'heatingorsystemtypeid_18.0', 'airconditioningtypeid_1.0', 'threequarterbathnbr', 'regionidcity', 'lotsizesquarefeet', 'fireplaceflag', 'propertylandusetypeid_263.0', 'airconditioningtypeid_11.0', 'bath_bed_ratio', 'airconditioningtypeid_5.0', 'unitcnt_237.0', 'airconditioningtypeid_13.0']\n",
      "Selection: forward, Model: XGBRegressor -> CV MAE: 196037.38 ± 2497.66, Test MAE: 196056.64, Features: ['calcsqft_squared', 'latitude', 'buildingqualitytypeid_9.0', 'buildingqualitytypeid_11.0', 'log_calculatedfinishedsquarefeet', 'buildingqualitytypeid_10.0', 'bedroomcnt', 'buildingqualitytypeid_12.0', 'longitude', 'is_new_home', 'hashottuborspa', 'propertylandusetypeid_246.0', 'poolcnt', 'heatingorsystemtypeid_24.0', 'buildingqualitytypeid_7.0', 'propertylandusetypeid_247.0', 'unitcnt_4.0', 'propertylandusetypeid_275.0', 'propertylandusetypeid_265.0', 'heatingorsystemtypeid_18.0', 'airconditioningtypeid_1.0', 'threequarterbathnbr', 'regionidcity', 'lotsizesquarefeet', 'fireplaceflag', 'propertylandusetypeid_263.0', 'airconditioningtypeid_11.0', 'bath_bed_ratio', 'airconditioningtypeid_5.0', 'unitcnt_237.0', 'airconditioningtypeid_13.0']\n",
      "Selection: backward, Model: LinearRegression -> CV MAE: 231247.02 ± 3526.14, Test MAE: 4574563762027.23, Features: ['bedroomcnt', 'calculatedfinishedsquarefeet', 'hashottuborspa', 'latitude', 'pooltypeid10', 'pooltypeid2', 'propertycountylandusecode', 'propertyzoningdesc', 'regionidcity', 'roomcnt', 'taxdelinquencyflag', 'taxdelinquencyyear', 'sqft_per_lot', 'calcsqft_squared', 'regionidcounty_2061.0', 'regionidcounty_3101.0', 'buildingqualitytypeid_3.0', 'buildingqualitytypeid_4.0', 'buildingqualitytypeid_5.0', 'buildingqualitytypeid_6.0', 'buildingqualitytypeid_7.0', 'buildingqualitytypeid_8.0', 'buildingqualitytypeid_9.0', 'buildingqualitytypeid_10.0', 'buildingqualitytypeid_11.0', 'buildingqualitytypeid_12.0', 'propertylandusetypeid_246.0', 'propertylandusetypeid_247.0', 'propertylandusetypeid_260.0', 'propertylandusetypeid_263.0', 'propertylandusetypeid_264.0', 'propertylandusetypeid_266.0', 'propertylandusetypeid_275.0', 'airconditioningtypeid_5.0', 'airconditioningtypeid_11.0', 'heatingorsystemtypeid_2.0', 'heatingorsystemtypeid_11.0', 'heatingorsystemtypeid_18.0', 'heatingorsystemtypeid_24.0', 'fips_6059.0', 'fips_6111.0', 'unitcnt_4.0', 'unitcnt_6.0', 'unitcnt_42.0', 'is_new_home']\n",
      "Selection: backward, Model: RandomForest -> CV MAE: 202298.54 ± 2921.06, Test MAE: 201619.33, Features: ['bedroomcnt', 'calculatedfinishedsquarefeet', 'hashottuborspa', 'latitude', 'pooltypeid10', 'pooltypeid2', 'propertycountylandusecode', 'propertyzoningdesc', 'regionidcity', 'roomcnt', 'taxdelinquencyflag', 'taxdelinquencyyear', 'sqft_per_lot', 'calcsqft_squared', 'regionidcounty_2061.0', 'regionidcounty_3101.0', 'buildingqualitytypeid_3.0', 'buildingqualitytypeid_4.0', 'buildingqualitytypeid_5.0', 'buildingqualitytypeid_6.0', 'buildingqualitytypeid_7.0', 'buildingqualitytypeid_8.0', 'buildingqualitytypeid_9.0', 'buildingqualitytypeid_10.0', 'buildingqualitytypeid_11.0', 'buildingqualitytypeid_12.0', 'propertylandusetypeid_246.0', 'propertylandusetypeid_247.0', 'propertylandusetypeid_260.0', 'propertylandusetypeid_263.0', 'propertylandusetypeid_264.0', 'propertylandusetypeid_266.0', 'propertylandusetypeid_275.0', 'airconditioningtypeid_5.0', 'airconditioningtypeid_11.0', 'heatingorsystemtypeid_2.0', 'heatingorsystemtypeid_11.0', 'heatingorsystemtypeid_18.0', 'heatingorsystemtypeid_24.0', 'fips_6059.0', 'fips_6111.0', 'unitcnt_4.0', 'unitcnt_6.0', 'unitcnt_42.0', 'is_new_home']\n",
      "Selection: backward, Model: XGBRegressor -> CV MAE: 202462.09 ± 3029.46, Test MAE: 201758.79, Features: ['bedroomcnt', 'calculatedfinishedsquarefeet', 'hashottuborspa', 'latitude', 'pooltypeid10', 'pooltypeid2', 'propertycountylandusecode', 'propertyzoningdesc', 'regionidcity', 'roomcnt', 'taxdelinquencyflag', 'taxdelinquencyyear', 'sqft_per_lot', 'calcsqft_squared', 'regionidcounty_2061.0', 'regionidcounty_3101.0', 'buildingqualitytypeid_3.0', 'buildingqualitytypeid_4.0', 'buildingqualitytypeid_5.0', 'buildingqualitytypeid_6.0', 'buildingqualitytypeid_7.0', 'buildingqualitytypeid_8.0', 'buildingqualitytypeid_9.0', 'buildingqualitytypeid_10.0', 'buildingqualitytypeid_11.0', 'buildingqualitytypeid_12.0', 'propertylandusetypeid_246.0', 'propertylandusetypeid_247.0', 'propertylandusetypeid_260.0', 'propertylandusetypeid_263.0', 'propertylandusetypeid_264.0', 'propertylandusetypeid_266.0', 'propertylandusetypeid_275.0', 'airconditioningtypeid_5.0', 'airconditioningtypeid_11.0', 'heatingorsystemtypeid_2.0', 'heatingorsystemtypeid_11.0', 'heatingorsystemtypeid_18.0', 'heatingorsystemtypeid_24.0', 'fips_6059.0', 'fips_6111.0', 'unitcnt_4.0', 'unitcnt_6.0', 'unitcnt_42.0', 'is_new_home']\n",
      "Selection: tree_importance, Model: LinearRegression -> CV MAE: 244354.89 ± 2803.54, Test MAE: 241351.99, Features: Index(['calcsqft_squared', 'log_calculatedfinishedsquarefeet',\n",
      "       'calculatedfinishedsquarefeet', 'longitude', 'latitude', 'regionidzip',\n",
      "       'yearbuilt', 'bath_bed_ratio', 'propertyzoningdesc', 'sqft_per_lot'],\n",
      "      dtype='object')\n",
      "Selection: tree_importance, Model: RandomForest -> CV MAE: 194570.07 ± 2764.86, Test MAE: 192833.03, Features: Index(['calcsqft_squared', 'log_calculatedfinishedsquarefeet',\n",
      "       'calculatedfinishedsquarefeet', 'longitude', 'latitude', 'regionidzip',\n",
      "       'yearbuilt', 'bath_bed_ratio', 'propertyzoningdesc', 'sqft_per_lot'],\n",
      "      dtype='object')\n",
      "Selection: tree_importance, Model: XGBRegressor -> CV MAE: 194554.85 ± 2495.53, Test MAE: 192640.22, Features: Index(['calcsqft_squared', 'log_calculatedfinishedsquarefeet',\n",
      "       'calculatedfinishedsquarefeet', 'longitude', 'latitude', 'regionidzip',\n",
      "       'yearbuilt', 'bath_bed_ratio', 'propertyzoningdesc', 'sqft_per_lot'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Helper function to select features using RandomForest importance\n",
    "def select_by_importance(X, y, n_features):\n",
    "    rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X, y)\n",
    "    importances = rf.feature_importances_\n",
    "    sorted_idx = np.argsort(importances)[::-1]\n",
    "    selected = X.columns[sorted_idx[:n_features]]\n",
    "    return selected\n",
    "\n",
    "# Define the three selection strategies\n",
    "selection_methods = {\n",
    "    'forward': lambda X, y: forward_feature_selection(\n",
    "        X, y, LinearRegression(),\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        tol=1e-4,\n",
    "        verbose=False\n",
    "    )[2],  # returns best_feature_set\n",
    "    'backward': lambda X, y: backward_feature_selection(\n",
    "        X, y, LinearRegression(),\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        tol=1e-4,\n",
    "        verbose=False\n",
    "    )[2],\n",
    "    'tree_importance': lambda X, y: select_by_importance(X, y, n_features=10)  # choose top 10 by importance\n",
    "}\n",
    "\n",
    "# Define the three models\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"XGBRegressor\": XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Evaluate each model with each selection method\n",
    "results = {}\n",
    "for sel_name, sel_func in selection_methods.items():\n",
    "    # Get selected features (list of column names)\n",
    "    selected_features = sel_func(X_train, y_train)\n",
    "    X_train_sel = X_train[selected_features]\n",
    "    X_test_sel  = X_test[selected_features]\n",
    "\n",
    "    # Scale features for models sensitive to scaling (LinearReg, XGB)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_sel)\n",
    "    X_test_scaled  = scaler.transform(X_test_sel)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # For tree-based models (RandomForest), scaling is optional;\n",
    "        # here we use scaled data for consistency\n",
    "        neg_mae = cross_val_score(\n",
    "            model,\n",
    "            X_train_scaled,\n",
    "            y_train,\n",
    "            scoring='neg_mean_absolute_error',\n",
    "            cv=RepeatedKFold(n_splits=5, n_repeats=5, random_state=42),\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        mae_scores = -neg_mae\n",
    "        mean_mae = mae_scores.mean()\n",
    "        std_mae = mae_scores.std()\n",
    "\n",
    "        # Fit on selected features and evaluate on the test set\n",
    "        if model_name in [\"LinearRegression\"]:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            preds = model.predict(X_test_scaled)\n",
    "        else:\n",
    "            # Tree models can use unscaled data, but we'll use scaled for consistency\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            preds = model.predict(X_test_scaled)\n",
    "\n",
    "        test_mae = mean_absolute_error(y_test, preds)\n",
    "\n",
    "        # Store results\n",
    "        results[(sel_name, model_name)] = {\n",
    "            \"cv_mae\": mean_mae,\n",
    "            \"cv_std\": std_mae,\n",
    "            \"test_mae\": test_mae,\n",
    "            \"features\": selected_features\n",
    "        }\n",
    "\n",
    "# Display results\n",
    "for (sel, model), metrics in results.items():\n",
    "    print(f\"Selection: {sel}, Model: {model} -> CV MAE: {metrics['cv_mae']:.2f} ± {metrics['cv_std']:.2f}, Test MAE: {metrics['test_mae']:.2f}, Features: {metrics['features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [3 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Three Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far, and pick a winner!\n",
    "\n",
    "1. For each of your three models:\n",
    "    - Choose your best engineered features and best selection of features as determined above. \n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, `Optuna`, etc. as you have practiced in previous homeworks. \n",
    "3. Decide on the best hyperparameters for each model, and for each run with repeated CV and record their final results:\n",
    "    - Report the **mean and standard deviation of CV MAE Score**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest – baseline:\n",
      "  Test MAE = 192619.77\n",
      "  Best Params = {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "XGBRegressor – baseline:\n",
      "  Test MAE = 193060.30\n",
      "  Best Params = {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'feature_weights': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "\n",
      "RandomForest – grid_search:\n",
      "  CV MAE = 191847.53\n",
      "  Test MAE = 190312.48\n",
      "  Best Params = {'max_depth': 20, 'max_features': 0.5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "XGBRegressor – grid_search:\n",
      "  CV MAE = 191316.87\n",
      "  Test MAE = 189124.87\n",
      "  Best Params = {'colsample_bytree': 0.6, 'learning_rate': 0.03, 'max_depth': 8, 'n_estimators': 400, 'subsample': 1.0}\n",
      "\n",
      "RandomForest – random_search:\n",
      "  CV MAE = 192554.96\n",
      "  Test MAE = 190848.79\n",
      "  Best Params = {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 0.5, 'max_depth': 20}\n",
      "\n",
      "XGBRegressor – random_search:\n",
      "  CV MAE = 192723.02\n",
      "  Test MAE = 191531.04\n",
      "  Best Params = {'subsample': 1.0, 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1, 'colsample_bytree': 0.6}\n",
      "\n",
      "RandomForest – manual_sweep:\n",
      "  CV MAE = 194037.20\n",
      "  Test MAE = 191887.62\n",
      "  Best Params = {'n_estimators': 200, 'max_depth': None}\n",
      "\n",
      "Best overall: XGBRegressor with grid_search (Test MAE = 189124.87)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "\n",
    "# Define hyperparameter grids for each model\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 0.5]\n",
    "}\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'max_depth': [4, 6, 8],\n",
    "    'learning_rate': [0.03, 0.1, 0.2],\n",
    "    'subsample': [0.7, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Define scorer (MAE)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Prepare a KFold CV splitter (no repeats here to keep time reasonable)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Container to store results\n",
    "results = {}\n",
    "\n",
    "# 1. Baseline (default hyperparameters)\n",
    "for model_name, model_cls in [('RandomForest', RandomForestRegressor), ('XGBRegressor', XGBRegressor)]:\n",
    "    model = model_cls(random_state=42, n_estimators=100)  # default (for XGB this sets learning_rate=0.1, etc.)\n",
    "    model.fit(X_train_sel, y_train)\n",
    "    preds = model.predict(X_test_sel)\n",
    "    test_mae = mean_absolute_error(y_test, preds)\n",
    "    results[(model_name, 'baseline')] = {'best_params': model.get_params(), 'cv_mae': None, 'test_mae': test_mae}\n",
    "\n",
    "# 2. GridSearchCV\n",
    "grid_models = {\n",
    "    'RandomForest': (RandomForestRegressor(random_state=42), rf_param_grid),\n",
    "    'XGBRegressor': (XGBRegressor(random_state=42, n_jobs=-1), xgb_param_grid)\n",
    "}\n",
    "\n",
    "for name, (model, param_grid) in grid_models.items():\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=mae_scorer,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid_search.fit(X_train_sel, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    preds = best_model.predict(X_test_sel)\n",
    "    test_mae = mean_absolute_error(y_test, preds)\n",
    "    cv_mae = -grid_search.best_score_\n",
    "    results[(name, 'grid_search')] = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'cv_mae': cv_mae,\n",
    "        'test_mae': test_mae\n",
    "    }\n",
    "\n",
    "# 3. RandomizedSearchCV\n",
    "random_models = {\n",
    "    'RandomForest': (RandomForestRegressor(random_state=42), rf_param_grid),\n",
    "    'XGBRegressor': (XGBRegressor(random_state=42, n_jobs=-1), xgb_param_grid)\n",
    "}\n",
    "\n",
    "for name, (model, param_grid) in random_models.items():\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,  # number of random combinations to try\n",
    "        scoring=mae_scorer,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    random_search.fit(X_train_sel, y_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "    preds = best_model.predict(X_test_sel)\n",
    "    test_mae = mean_absolute_error(y_test, preds)\n",
    "    cv_mae = -random_search.best_score_\n",
    "    results[(name, 'random_search')] = {\n",
    "        'best_params': random_search.best_params_,\n",
    "        'cv_mae': cv_mae,\n",
    "        'test_mae': test_mae\n",
    "    }\n",
    "\n",
    "# 4. Manual parameter sweep (simple custom search):\n",
    "# We manually iterate over a small grid for demonstration.\n",
    "def manual_sweep(model_cls, param_grid, X, y, cv_splitter):\n",
    "    best_params = None\n",
    "    best_mae = float('inf')\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            # only a subset of parameters for brevity\n",
    "            params = {'n_estimators': n_estimators, 'max_depth': max_depth}\n",
    "            model = model_cls(random_state=42, **params)\n",
    "            maes = []\n",
    "            for train_idx, val_idx in cv_splitter.split(X):\n",
    "                X_tr, X_val = X[train_idx], X[val_idx]\n",
    "                y_tr, y_val = y[train_idx], y[val_idx]\n",
    "                model.fit(X_tr, y_tr)\n",
    "                preds = model.predict(X_val)\n",
    "                maes.append(mean_absolute_error(y_val, preds))\n",
    "            mean_mae = np.mean(maes)\n",
    "            if mean_mae < best_mae:\n",
    "                best_mae = mean_mae\n",
    "                best_params = params\n",
    "    return best_params, best_mae\n",
    "\n",
    "# Example manual sweep on RandomForest for n_estimators and max_depth\n",
    "rf_best_params, rf_best_cv_mae = manual_sweep(RandomForestRegressor, {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10]\n",
    "}, X_train_sel.values, y_train.values, cv)\n",
    "\n",
    "# Train final model with best parameters and evaluate on test set\n",
    "rf_manual_model = RandomForestRegressor(random_state=42, **rf_best_params)\n",
    "rf_manual_model.fit(X_train_sel, y_train)\n",
    "rf_manual_preds = rf_manual_model.predict(X_test_sel)\n",
    "rf_manual_test_mae = mean_absolute_error(y_test, rf_manual_preds)\n",
    "results[('RandomForest', 'manual_sweep')] = {\n",
    "    'best_params': rf_best_params,\n",
    "    'cv_mae': rf_best_cv_mae,\n",
    "    'test_mae': rf_manual_test_mae\n",
    "}\n",
    "\n",
    "# Summarize and identify the best combination\n",
    "for key, metrics in results.items():\n",
    "    model_name, method = key\n",
    "    print(f\"{model_name} – {method}:\")\n",
    "    if metrics['cv_mae'] is not None:\n",
    "        print(f\"  CV MAE = {metrics['cv_mae']:.2f}\")\n",
    "    print(f\"  Test MAE = {metrics['test_mae']:.2f}\")\n",
    "    print(f\"  Best Params = {metrics['best_params']}\\n\")\n",
    "\n",
    "# Identify the best overall method by lowest test MAE\n",
    "best_key = min(results, key=lambda k: results[k]['test_mae'])\n",
    "print(f\"Best overall: {best_key[0]} with {best_key[1]} (Test MAE = {results[best_key]['test_mae']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [3 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Final Model and Design Reassessment [6 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model.  You’ll also consolidate and present the key code used to run your model on the preprocessed dataset.\n",
    "**Requirements:**\n",
    "\n",
    "- Decide one your final model among the three contestants. \n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset, reporting\n",
    "\n",
    "    - Mean and standard deviation of CV MAE Score.\n",
    "    \n",
    "    - Test score on held-out test set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test MAE (XGBRegressor with tuned params): 189,124.87\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters from grid search\n",
    "best_params = {\n",
    "    'n_estimators': 400,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.03,\n",
    "    'subsample': 1.0,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Initialize and train the final model\n",
    "final_model = XGBRegressor(**best_params)\n",
    "final_model.fit(X_train_sel, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "final_predictions = final_model.predict(X_test_sel)\n",
    "\n",
    "# Evaluate with MAE\n",
    "final_test_mae = mean_absolute_error(y_test, final_predictions)\n",
    "print(f\"Final Test MAE (XGBRegressor with tuned params): {final_test_mae:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Discussion [8 pts]\n",
    "\n",
    "In this final step, your goal is to synthesize your entire modeling process and assess how your earlier decisions influenced the outcome. Please address the following:\n",
    "\n",
    "1. Model Selection:\n",
    "- Clearly state which model you selected as your final model and why.\n",
    "\n",
    "- What metrics or observations led you to this decision?\n",
    "\n",
    "- Were there trade-offs (e.g., interpretability vs. performance) that influenced your choice?\n",
    "\n",
    "2. Revisiting an Early Decision\n",
    "\n",
    "- Identify one specific preprocessing or feature engineering decision from Milestone 1 (e.g., how you handled missing values, how you scaled or encoded a variable, or whether you created interaction or polynomial terms).\n",
    "\n",
    "- Explain the rationale for that decision at the time: What were you hoping it would achieve?\n",
    "\n",
    "- Now that you've seen the full modeling pipeline and final results, reflect on whether this step helped or hindered performance. Did you keep it, modify it, or remove it?\n",
    "\n",
    "- Justify your final decision with evidence—such as validation scores, visualizations, or model diagnostics.\n",
    "\n",
    "3. Lessons Learned\n",
    "\n",
    "- What insights did you gain about your dataset or your modeling process through this end-to-end workflow?\n",
    "\n",
    "- If you had more time or data, what would you explore next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
